





import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from scipy import stats
import numpy as np





inactivity_source_path = Path("Resources/500_Cities__No_leisure-time_physical_activity_among_adults_aged___18_years_20240314.csv")
obesity_source_path = Path("Resources/500_Cities__Obesity_among_adults_aged___18_years_20240318.csv")
inactivity_source_data = pd.read_csv(inactivity_source_path)
obesity_source_data = pd.read_csv(obesity_source_path)








filtered_inactivity_source_data = inactivity_source_data[["StateDesc", "Data_Value", "PopulationCount"]]
percentage = filtered_inactivity_source_data["Data_Value"]*0.01
inactivity_population = percentage * filtered_inactivity_source_data["PopulationCount"]
inactivity_population = round(inactivity_population)
inactivity_population = inactivity_population.rename("InactivityPopulation")
filtered_inactivity_source_data = pd.concat([filtered_inactivity_source_data, inactivity_population],axis = 1)
filtered_inactivity_source_data = filtered_inactivity_source_data[filtered_inactivity_source_data["Data_Value"].notna()]
calculation_data = filtered_inactivity_source_data.groupby(by = "StateDesc").sum()
percentage = calculation_data["InactivityPopulation"]/calculation_data["PopulationCount"]
state_filtered_inactivity_source_data = percentage.rename("Inactivity_Percentage")


filtered_obesity_source_data = obesity_source_data[["StateDesc", "Data_Value", "PopulationCount"]]
percentage = filtered_obesity_source_data["Data_Value"]*0.01
obesity_population = percentage * filtered_obesity_source_data["PopulationCount"]
obesity_population = round(obesity_population)
obesity_population = obesity_population.rename("ObesityPopulation")
filtered_obesity_source_data = pd.concat([filtered_obesity_source_data, obesity_population],axis = 1)
filtered_obesity_source_data = filtered_obesity_source_data[filtered_obesity_source_data["Data_Value"].notna()]
calculation_data = filtered_obesity_source_data.groupby(by = "StateDesc").sum()
percentage = calculation_data["ObesityPopulation"]/calculation_data["PopulationCount"]
state_filtered_obesity_source_data = percentage.rename("Obesity_Percentage")


state_inactivity_obesity_source_data = pd.merge(state_filtered_obesity_source_data, state_filtered_inactivity_source_data,how = "inner", on = ["StateDesc"])
state_inactivity_obesity_source_data = state_inactivity_obesity_source_data.drop(index = ["United States","District of C"])
state_inactivity_obesity_source_data_sorted_by_inactivity = state_inactivity_obesity_source_data.sort_values(by = ["Inactivity_Percentage"], ascending = False)
state_inactivity_obesity_source_data_sorted_by_obesity = state_inactivity_obesity_source_data.sort_values(by = ["Obesity_Percentage"], ascending = False)
state_inactivity_obesity_source_data.to_csv("state_inactivity_obesity_source_data.csv")
state_inactivity_obesity_source_data.head()


plt.figure(figsize=(10, 6))
plt.bar(state_inactivity_obesity_source_data_sorted_by_inactivity.index, state_inactivity_obesity_source_data_sorted_by_inactivity['Inactivity_Percentage'])
plt.xticks(fontsize = 10, rotation = "vertical")
plt.ylabel("Inactivity_Percentage")
plt.xlabel("State")
plt.title("Inactivity Rates (due to no leisure time) of each state")
plt.savefig("Inactivity Rates (due to no leisure time) of each state", bbox_inches = "tight")
plt.show()


## Bar Plots by Percentage


plt.figure(figsize=(10, 6))
plt.bar(state_inactivity_obesity_source_data_sorted_by_obesity.index, state_inactivity_obesity_source_data_sorted_by_obesity['Obesity_Percentage'])
plt.xticks(fontsize = 10, rotation = "vertical")
plt.ylabel("Obesity_Percentage")
plt.xlabel("State")
plt.title("Obesity Rates of each state")
plt.savefig("Obesity Rates of each state", bbox_inches = "tight")
plt.show()


x_label = state_inactivity_obesity_source_data.index
obesity_data = state_inactivity_obesity_source_data["Obesity_Percentage"]
inactivity_data = state_inactivity_obesity_source_data["Inactivity_Percentage"]
x_axis = np.arange(len(x_label))
plt.figure(figsize=(10, 6))
plt.bar(x_axis-0.2, obesity_data, 0.4, label = "Obesity Percentage")
plt.bar(x_axis+0.2, inactivity_data, 0.4, label = "Inactivity Percentage")
plt.xticks(x_axis, x_label, rotation = 90)
plt.title("Inactivity Rates and Obesity Rates of each state")
plt.xlabel("States")
plt.ylabel("Percentage")
plt.legend()
plt.savefig("Inactivity Rates and Obesity Rates of each state", bbox_inches = "tight")
plt.show()


stat_state_inactivity_obesity_source_data_sorted_by_inactivity = state_inactivity_obesity_source_data_sorted_by_inactivity.describe()
stat_state_inactivity_obesity_source_data_sorted_by_inactivity





plt.scatter(inactivity_data, obesity_data)
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(inactivity_data, obesity_data)
p_fit = p_slope*inactivity_data+p_int
plt.plot(inactivity_data, p_fit, color = "red")
plt.title("State Rates")
plt.xlabel("Inactivity Rates")
plt.ylabel("Obesity Rates")
plt.figtext(0.4, 0.15, f"r = {str(p_r)}")
plt.savefig("State Rates", bbox_inches = "tight")
plt.show()





most_populated_state_inactivity_obesity_source_data_sorted_by_inactivity = state_inactivity_obesity_source_data_sorted_by_inactivity.loc[["California","Texas","Florida","New York","Pennsylvania"]]
x_label = most_populated_state_inactivity_obesity_source_data_sorted_by_inactivity.index
x_axis = np.arange(len(x_label))
plt.figure(figsize=(10, 6))
plt.bar(x_axis-0.2, most_populated_state_inactivity_obesity_source_data_sorted_by_inactivity["Obesity_Percentage"], 0.4, label = "Obesity Percentage")
plt.bar(x_axis+0.2, most_populated_state_inactivity_obesity_source_data_sorted_by_inactivity["Inactivity_Percentage"], 0.4, label = "Inactivity Percentage")
plt.xticks(x_axis, x_label, rotation = 90)
plt.title("Inactivity Rates and Obesity Rates of top 5 most populated state")
plt.xlabel("States")
plt.ylabel("Percentage")
plt.legend()
plt.savefig("Inactivity Rates and Obesity Rates of top 5 most populated state", bbox_inches = "tight")
plt.show()





filtered_city_inactivity_data = inactivity_source_data[["StateAbbr","CityName","Data_Value","PopulationCount"]]
filtered_city_inactivity_data = filtered_city_inactivity_data[filtered_city_inactivity_data["Data_Value"].notna()]
filtered_city_inactivity_data = pd.concat([filtered_city_inactivity_data,inactivity_population],axis = 1)
filtered_city_inactivity_data = filtered_city_inactivity_data.groupby(by = "CityName")[["PopulationCount","InactivityPopulation"]].mean()
city_inactivity_percentage = filtered_city_inactivity_data["InactivityPopulation"]/filtered_city_inactivity_data["PopulationCount"]
city_inactivity_percentage.name = "CityInactivityPercentage"
filtered_city_inactivity_data = pd.concat([filtered_city_inactivity_data, city_inactivity_percentage], axis = 1)
stat_filtered_city_inactivity_data = filtered_city_inactivity_data.describe()
filtered_city_inactivity_data.head()


stat_filtered_city_inactivity_data


filtered_city_obesity_data = obesity_source_data[["StateAbbr", "CityName" ,"Data_Value","PopulationCount"]]
filtered_city_obesity_data = filtered_city_obesity_data[filtered_city_obesity_data["Data_Value"].notna()]
filtered_city_obesity_data = pd.concat([filtered_city_obesity_data, obesity_population],axis = 1)
filtered_city_obesity_data = filtered_city_obesity_data.groupby(by = "CityName")[["PopulationCount","ObesityPopulation"]].mean()
city_obesity_percentage = filtered_city_obesity_data["ObesityPopulation"]/filtered_city_obesity_data["PopulationCount"]
city_obesity_percentage.name = "CityObesityPercentage"
filtered_city_obesity_data = pd.concat([filtered_city_obesity_data, city_obesity_percentage], axis = 1)
stat_filtered_city_obesity_data = filtered_city_obesity_data.describe()
filtered_city_obesity_data.head()


stat_filtered_city_obesity_data





plt.scatter(filtered_city_inactivity_data["CityInactivityPercentage"], filtered_city_obesity_data["CityObesityPercentage"])
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(filtered_city_inactivity_data["CityInactivityPercentage"], filtered_city_obesity_data["CityObesityPercentage"])
p_fit = p_slope*filtered_city_inactivity_data["CityInactivityPercentage"]+p_int
plt.plot(filtered_city_inactivity_data["CityInactivityPercentage"], p_fit, color = "red")
plt.title("City Rates")
plt.xlabel("Inactivity Rates")
plt.ylabel("Obesity Rates")
plt.figtext(.45,0.15, f"r = {p_r}")
plt.savefig('City Rates')
plt.show()


fig, (ax1 ,ax2) = plt.subplots(2)
ax1.hist(filtered_city_obesity_data["CityObesityPercentage"], bins=100, color = "Orange")
ax1.set_ylabel("City Obesity Rate Counts")
ax2.hist(filtered_city_inactivity_data["CityInactivityPercentage"], bins=100)
ax2.set_ylabel("City Inactivity Rates Counts")
fig.savefig("Distribution of rates")
fig.show


plt.scatter(filtered_city_inactivity_data["InactivityPopulation"], filtered_city_obesity_data["ObesityPopulation"])
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(filtered_city_inactivity_data["InactivityPopulation"], filtered_city_obesity_data["ObesityPopulation"])
p_fit = p_slope*filtered_city_inactivity_data["InactivityPopulation"]+p_int
plt.plot(filtered_city_inactivity_data["InactivityPopulation"], p_fit, color = "red")
plt.title("City Population")
plt.xlabel("Inactivity Population")
plt.ylabel("Obesity Population")
plt.figtext(0.5, 0.15, f'r = {p_r}')
plt.savefig("City Population")
plt.show()





county_data_source = Path("Resources/uscities.csv")
county_data = pd.read_csv(county_data_source)
county_data = county_data[["city", "county_name"]]
county_data = county_data.rename(columns = {"city": "CityName", "county_name": "CountyName"})
county_data = county_data.drop_duplicates(subset = ["CityName"])


county_inactivity_data = filtered_city_inactivity_data.merge(county_data, on = "CityName", how = "left")
county_inactivity_data = county_inactivity_data.groupby(by = "CountyName")[["PopulationCount", "InactivityPopulation"]].sum()
county_inactivity_percentage = county_inactivity_data["InactivityPopulation"]/county_inactivity_data["PopulationCount"]
county_inactivity_percentage.name = "CountyInactivityPercentage"
county_inactivity_data = county_inactivity_data.merge(county_inactivity_percentage, on = "CountyName", how = "left")


county_obesity_data = filtered_city_obesity_data.merge(county_data, on = "CityName", how = "left")
county_obesity_data = county_obesity_data.groupby(by = "CountyName")[["PopulationCount", "ObesityPopulation"]].sum()
county_obesity_percentage = county_obesity_data["ObesityPopulation"]/county_obesity_data["PopulationCount"]
county_obesity_percentage.name = "CountyObesityPercentage"
county_obesity_data = county_obesity_data.merge(county_obesity_percentage, on = "CountyName", how = "left")


plt.scatter(county_inactivity_data["InactivityPopulation"],county_obesity_data["ObesityPopulation"])
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(county_inactivity_data["InactivityPopulation"],county_obesity_data["ObesityPopulation"])
p_fit = p_slope*county_inactivity_data["InactivityPopulation"]+p_int
plt.plot(county_inactivity_data["InactivityPopulation"], p_fit, color = "red")
plt.title("County Population")
plt.ylabel("Obesity Population")
plt.xlabel("Inactivity Population")
plt.figtext(0.5, 0.15, f'r = {p_r}')
plt.savefig("County Population")
plt.show()


plt.scatter(county_inactivity_data["CountyInactivityPercentage"],county_obesity_data["CountyObesityPercentage"])
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(county_inactivity_data["CountyInactivityPercentage"],county_obesity_data["CountyObesityPercentage"])
p_fit = p_slope*county_inactivity_data["CountyInactivityPercentage"]+p_int
plt.plot(county_inactivity_data["CountyInactivityPercentage"], p_fit, color = "red")
plt.title("County Rates")
plt.xlabel("Inactivity Rates")
plt.ylabel("Obesity Rates")
plt.figtext(0.5, 0.15, f'r = {p_r}')
plt.savefig("County Rates")
plt.show()





import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt 
import hvplot.pandas
from scipy.stats import linregress



# Reads the data that will be used 

data_csv = pd.read_csv("Resources/500_Cities__Coronary_heart_disease_among_adults_aged___18_years_20240317.csv")

data_csv.head()


# Shows all the unique values under the State Description column

data_csv['StateDesc'].unique()


# Drops District of Columbia from the State Description column

states_df = data_csv.drop(data_csv.loc[data_csv['StateDesc'] == 'District of C', :].index)

states_df.head()


# Drops United States from the State Description column
states_df = states_df.drop(states_df.loc[states_df['StateDesc'] == 'United States', :].index)

# Shows the unique amount of values in the State Description column 
states_df['StateDesc'].nunique()


states_df.columns


# Makes dataframe containing only the most important columns 

reduced_states_df = states_df[['StateDesc', 'CityName', 'Data_Value_Type', 'Data_Value', 'PopulationCount', 'GeoLocation']]

reduced_states_df.head()


# Removes any rows with NA in "Data_Value" column
reduced_states_df = reduced_states_df[reduced_states_df["Data_Value"].notna()]




# Copies dataframe and extracts Longitude and latitude data from dataframe
reduced_coordinates_df = reduced_states_df.copy()
reduced_coordinates_df[["Lat","Long"]] = reduced_coordinates_df['GeoLocation'].str.split(",", expand = True)
reduced_coordinates_df['Lat'] = reduced_coordinates_df['Lat'].str.removeprefix("(")
reduced_coordinates_df['Long'] = reduced_coordinates_df['Long'].str.removesuffix(")")


# Converts longitude and latitude columns into float and plots on map 
reduced_coordinates_df['Long'] = reduced_coordinates_df['Long'].astype(float) 
reduced_coordinates_df['Lat'] = reduced_coordinates_df['Lat'].astype(float)
reduced_coordinates_df.hvplot.points('Long','Lat', geo = True, tiles = 'OSM', size = 'Data_Value')



# Groups by state and finds the average latitude, longitude, and data value for each
state_reduced_coordinates = reduced_coordinates_df.groupby('StateDesc')[['Data_Value', 'Lat','Long']].mean()

# Converts it to dataframe
state_reduced_coordinates_df = state_reduced_coordinates.reset_index()

# Creates new column in this dataframe
state_reduced_coordinates_df['category'] = " "
state_reduced_coordinates_df['legend_values'] = " "

# Sorts the dataframe in descending order based on data values 
state_reduced_coordinates_df = state_reduced_coordinates_df.sort_values(by = "Data_Value", ascending = False)
counter = 1

# Color codes the top 10 highest data values as red and the rest as green
for index, row in state_reduced_coordinates_df.iterrows():
    if counter <= 10:
        state_reduced_coordinates_df.loc[index, 'category'] = 'red'
        state_reduced_coordinates_df.loc[index, 'legend_values'] = 'top 10 data values'
        counter = counter + 1
    else:
        state_reduced_coordinates_df.loc[index, 'category'] = 'green'
        state_reduced_coordinates_df.loc[index, 'legend_values'] = 'not in top 10 data values'
        counter = counter + 1

# Plots the  points on a map 
state_reduced_coordinates_df.hvplot.points('Long','Lat', by = 'legend_values', legend = 'bottom_left',color='category', geo = True,
                                     tiles = 'OSM', hover_cols = ['Long','Lat','StateDesc','category'])


# Groups by each state and finds the average data value for each state
average_value = reduced_states_df.groupby("StateDesc")['Data_Value'].mean()

average_df = average_value.reset_index()

# Makes dataframe with values sorted in descending order 
sorted_average_df = average_df.sort_values(by = 'Data_Value',ascending = False)

sorted_average_df.head()


# Bar plot of the average data value for each state

plt.figure(figsize=(10, 6))
plt.bar(sorted_average_df['StateDesc'], sorted_average_df['Data_Value'])
plt.xticks(fontsize = 10, rotation = "vertical")
plt.ylabel("Mean Data Value")
plt.xlabel("State")
plt.title("Average data value for each state")
plt.show()


# Groups by state and the data value type and finds the average data value for each unique grouping

state_data_type = reduced_states_df.groupby(['StateDesc','Data_Value_Type'])['Data_Value'].mean()
state_data_type


# Converts the series into a dataframe

state_data_type_df = state_data_type.reset_index()
state_data_type_df.head()


# Makes a dataframe for each data value type (age adjusted and crude) 

age_adjusted_df = state_data_type_df.loc[state_data_type_df['Data_Value_Type'] == 'Age-adjusted prevalence',:]
crude_df = state_data_type_df.loc[state_data_type_df['Data_Value_Type'] == 'Crude prevalence',:]

crude_df.head()


# Plots a bar of average values for each data value type per state 

state_names = crude_df['StateDesc']

crude_bar = crude_df['Data_Value']
age_adjusted_bar = age_adjusted_df['Data_Value']
  
X_axis = np.arange(len(state_names)) 
  
plt.figure(figsize=(14, 6))
plt.bar(X_axis - 0.2, crude_bar, 0.3, label = 'crude') 
plt.bar(X_axis + 0.2, age_adjusted_bar, 0.3, label = 'age adjusted') 
plt.legend()  
plt.xticks(X_axis, state_names, fontsize = 10, rotation = "vertical") 
plt.xlabel("State")
plt.ylabel("Average Data Value")
plt.title("Average Data Value for each data value type per state")

plt.show()


# Saves newly created dataframe in Resources folder to be used elsewhere
states_df.to_csv("Resources/states_df.csv", index = False)


# Reads in the obesity and inactivity dataframe
obesity_data = pd.read_csv("Resources/state_inactivity_obesity_source_data.csv")

obesity_data['Obesity_Percentage'] = obesity_data['Obesity_Percentage'].astype(float) * 100
obesity_data['Inactivity_Percentage'] = obesity_data['Inactivity_Percentage'].astype(float) * 100
obesity_data.head()


obesity_inactivity_data_value_df = pd.merge(average_df,obesity_data, on = 'StateDesc')

obesity_inactivity_data_value_df.head()


# Converts the data into float values
obesity_inactivity_data_value_df['Data_Value'] = obesity_inactivity_data_value_df['Data_Value'].astype(float)
obesity_inactivity_data_value_df['Obesity_Percentage'] = obesity_inactivity_data_value_df['Obesity_Percentage'].astype(float)

# Shows a scatterplot between heart disease data value and obesity
plt.scatter(obesity_inactivity_data_value_df['Data_Value'], obesity_inactivity_data_value_df['Obesity_Percentage'])
plt.xlabel('Heart Disease Percentage (%)')
plt.ylabel('Obesity Percentage (%)')
plt.title('Obesity vs. Heart Disease')

# Calculates slope, intercept, rval, pval, and std
slope, intercept, rval, pval, std = linregress(obesity_inactivity_data_value_df['Data_Value'],obesity_inactivity_data_value_df['Obesity_Percentage'])


# Creates strings that will be shown in the plot
equation = f'y = {slope:.2f}x + {intercept:.2f}'
r_value_text = f'r-value = {rval:.2f}'

# Creates an array of the maximum and minimum x-values
x_min = obesity_inactivity_data_value_df['Data_Value'].min()
x_max = obesity_inactivity_data_value_df['Data_Value'].max()
x_values = x_min, x_max
x_values_array = np.array(x_values)

# Predicts y-values using the linear regression equation
y_pred = slope * x_values_array + intercept

# Plots the linear regression line and shows the string values determined above
plt.plot(x_values_array, y_pred, c = 'r')
plt.text(x = 7.0, y = 30, fontsize = 14, s = equation, c = 'r')
plt.text(x = 7.0, y = 28, fontsize = 14, s = r_value_text, c = 'r') 
plt.show()






# Converts the data into float values
obesity_inactivity_data_value_df['Data_Value'] = obesity_inactivity_data_value_df['Data_Value'].astype(float)
obesity_inactivity_data_value_df['Inactivity_Percentage'] = obesity_inactivity_data_value_df['Inactivity_Percentage'].astype(float)

# Shows a scatterplot between heart disease data value and inactivity
plt.scatter(obesity_inactivity_data_value_df['Data_Value'], obesity_inactivity_data_value_df['Inactivity_Percentage'])
plt.xlabel('Heart Disease Percentage (%)')
plt.ylabel('Inactivity Percentage (%)')
plt.title('Inactivity vs. Heart Disease')

# Calculates slope, intercept, rval, pval, and std
slope, intercept, rval, pval, std = linregress(obesity_inactivity_data_value_df['Data_Value'],obesity_inactivity_data_value_df['Inactivity_Percentage'])


# Creates strings that will be shown in the plot
equation = f'y = {slope:.2f}x + {intercept:.2f}'
r_value_text = f'r-value = {rval:.2f}'

# Creates an array of the maximum and minimum x-values
x_min = obesity_inactivity_data_value_df['Data_Value'].min()
x_max = obesity_inactivity_data_value_df['Data_Value'].max()
x_values = x_min, x_max
x_values_array = np.array(x_values)

# Predicts y-values using the linear regression equation
y_pred = slope * x_values_array + intercept

# Plots the linear regression line and shows the string values determined above
plt.plot(x_values_array, y_pred, c = 'r')
plt.text(x = 7.0, y = 27, fontsize = 14, s = equation, c = 'r')
plt.text(x = 7.0, y = 25, fontsize = 14, s = r_value_text, c = 'r') 
plt.show()









# Dependencies
# Note that an API key is required for the Government Census data
from api_keys import census_key
import requests
import numpy as np
import pandas as pd
from census import Census
import matplotlib.pyplot as plt
from pathlib import Path
import scipy.stats as st


# Creates an instance of the Census library
income_census = Census(
    census_key,
    year = 2017
)





# Runs Census search to retrieve data on all states
state_income_census_data = income_census.acs5.get(
    (
        "NAME",
        "B19301_001E"
    ),
    {'for': 'state:*'}
)

# Converts data to DataFrame
state_income_census_pd = pd.DataFrame(state_income_census_data)

# Renames the columns
state_income_census_pd = state_income_census_pd.rename(
    columns = {"B19301_001E": "Per Capita Income"
              }
)


# Creates a dictionary of states and their per capita income
income_state_dict = state_income_census_pd[['NAME', 'Per Capita Income']].set_index('NAME').to_dict()

# Removes the per capita income index from the dictionary so the dictionary can later be applied to the heart disease DataFrame
income_state_dict = income_state_dict.pop('Per Capita Income')


# Retrieves and reads .csv data
heart_income_csv = Path('Resources/500_Cities__Coronary_heart_disease_among_adults_aged___18_years_20240317.csv')
heart_income_csv = pd.read_csv(heart_income_csv)

# Isolates the columns that are relevant to our analysis
heart_income_df = heart_income_csv[['StateDesc', 'CityName', 'Data_Value', 'UniqueID', 'PopulationCount', 'Data_Value_Type']]

# Sorts our DataFrame by population and data value type in descending order
heart_income_df = heart_income_df.sort_values(['PopulationCount', 'Data_Value_Type'], ascending=False)

# Removes duplicated city IDs, keeping the first occurence of each ID that has the highest population
# and crude prevalence data type
heart_income_df = heart_income_df.drop_duplicates(subset=['UniqueID'])
heart_income_df = heart_income_df[['StateDesc', 'CityName', 'Data_Value']]

heart_income_df


# Adds a duplicate StateDesc column
heart_income_df['Per Capita Income'] = heart_income_df.loc[:, 'StateDesc']

# Uses the dictionary to replace each state name in the duplicated column with each state's respective per capita income values
# Renames the duplicated column to Per Capita Income
heart_income_df['Per Capita Income'] = heart_income_df['Per Capita Income'].map(income_state_dict)

# Drops all rows that contain null values
heart_income_df = heart_income_df.dropna()
heart_income_df


# Calculates the correlation coefficient between income and heart disease rates
income_correlation = st.pearsonr(heart_income_df['Per Capita Income'], heart_income_df['Data_Value'])

# Generates a scatter plot of income vs. heart disease rates
plt.scatter(heart_income_df['Per Capita Income'], heart_income_df['Data_Value'])
plt.xlabel('Statewide Per Capita Income (USD)')
plt.ylabel('% Adults diagnosed with Heart Disease')
plt.title('Percentage of Adults Aged 18+ Diagnosed with Coronary Heart Disease')

# Prints our analysis
print(f'The correlation coefficient between per capita income and heart disease is {round(income_correlation[0],2)}.')
print(f'There is no correlation between income and heart disease rates.')
plt.show()





# Runs Census search to retrieve data on all counties in the US
county_income_census_data = income_census.acs5.get(
    (
        "NAME",
        "B19301_001E"
    ),
    {'for': 'county:*'}
)

# Converts data to DataFrame
county_income_census_pd = pd.DataFrame(county_income_census_data)

# Renames the columns
county_income_census_pd = county_income_census_pd.rename(
    columns = {"B19301_001E": "Per Capita Income"
              }
)
county_income_census_pd


# Splits the county names of the county DataFrame to isolate the county's name and state
county_income_census_pd[['NAME', 'state']] = county_income_census_pd['NAME'].str.split(' County, ', n=1, expand=True)
county_income_census_pd


# Isolates the relevant columns for our analysis and renames columns to prepare for merging
county_income_census_pd = county_income_census_pd[['NAME', 'Per Capita Income', 'state']].rename(columns={'NAME':'county_name',
                                                                                                         'state':'state_name'})
                                           


# Generates a DataFrame containing all cities in the US and the counties/states they belong to
city_county_csv = Path('Resources/uscities.csv')
city_county_csv = pd.read_csv(city_county_csv)
city_county_df = city_county_csv[['city', 'state_name', 'county_name']]
city_county_df


# Generates a DataFrame containing all of the cities included in the heart disease study and
# their respective states
heart_cities_df = heart_income_csv[['CityName', 'StateDesc']].drop_duplicates(subset=['CityName', 'StateDesc'])

# Renames columns for merging
heart_cities_df = heart_cities_df.rename(columns={'CityName':'city', 'StateDesc':'state_name'})

# Merges our DataFrames to produce a new DataFrame containing only our studied cities, their states,
# and their counties
merged_heart_cities_df = heart_cities_df.merge(city_county_df, how='left')

# Checks our merged DataFrame for null values
# https://stackoverflow.com/questions/14247586/how-to-select-rows-with-one-or-more-nulls-from-a-pandas-dataframe-without-listin
merged_heart_cities_df[merged_heart_cities_df.isnull().any(axis=1)]


# Identifies the city name used in city_county_df so that the null value above can be resolved
city_county_df.loc[city_county_df['city']=='San Buenaventura']


# Identifies the city name used in city_county_df so that the null value above can be resolved
city_county_df.loc[city_county_df['city']=='Boise']


# Replaces the mismatched city names
heart_cities_df = heart_cities_df.replace('San Buenaventura (Ventura)', 'San Buenaventura').replace('Boise City', 'Boise')

# Drops the remaining row with a null values
heart_cities_df = heart_cities_df.dropna()

# Creates a new, corrected DataFrame with no null values
corrected_heart_cities_df = heart_cities_df.merge(city_county_df, how='left')


# Merges the DataFrame containing the studied cities/states/counties and the DataFrame
# containing the income values for each county
reduced_county_income = corrected_heart_cities_df.merge(county_income_census_pd, how='left')

# Drops the rows with null values since we don't have per capita income data for these locations
reduced_county_income = reduced_county_income.dropna()

reduced_county_income


# Creates a dictionary from the DataFrame created above which contains 
# a city key and a per capita income value
county_income_dict = reduced_county_income[['city', 'Per Capita Income']].set_index('city').to_dict()

# Removes 'Per Capita Income' index from the dictionary for later processing
county_income_dict = county_income_dict.pop('Per Capita Income')


# References the DataFrame we created in the statewide summary
income_summary = heart_income_df[['StateDesc', 'CityName', 'Data_Value']]

# Duplicates the CityName column of this DataFrame and renames the duplicated column 'Per Capita Income'
income_summary['Per Capita Income'] = income_summary.loc[:, 'CityName']

# Uses the dictionary created above to replace the values in the duplicated row with
# each city's countywide per capita income
income_summary['Per Capita Income'] = income_summary['Per Capita Income'].map(county_income_dict)

# Drops the null values in the DataFrame where per capita income data was unavailable
income_summary = income_summary.dropna()

income_summary


# Calculates the correlation coefficient between income and heart disease rates
county_income_correlation = st.pearsonr(income_summary['Per Capita Income'], income_summary['Data_Value'])

# Generates a scatter plot of income vs. heart disease rates
plt.scatter(income_summary['Per Capita Income'], income_summary['Data_Value'])
plt.xlabel('Per Capita Income (USD) by County')
plt.ylabel('% Adults diagnosed with Heart Disease')
plt.title('Percentage of Adults Aged 18+ Diagnosed with Coronary Heart Disease')

# Prints our analysis
print(f'The correlation coefficient between per capita income and heart disease is {round(income_correlation[0],2)}.')
print(f'There is no correlation between income and heart disease rates.')
plt.show()








import pandas as pd
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats


#file to load
mentalHealth_df = Path("Resources/500_Cities_Mental_health_.csv")
heartDisease_df = Path("Resources/states_df.csv")

heartDisease_df = pd.read_csv(heartDisease_df)
mentalHealth_df = pd.read_csv(mentalHealth_df)






# Shows all the unique values under the State Description column
mentalHealth_df['StateDesc'].unique()
# Drop Values in Array that aren't states
mental_states = mentalHealth_df.drop(mentalHealth_df.loc[mentalHealth_df['StateDesc'] == 'District of C', :].index)
mental_states = mental_states.drop(mentalHealth_df.loc[mentalHealth_df['StateDesc'] == 'United States', :].index)





#Select Index
mental_health = mental_states[["StateDesc", 'CityName', "Data_Value", "PopulationCount","Measure"]] 

#convert data value (%) to real number and add to dataframe
percent = mental_health["Data_Value"]*0.01
poormentalHealth = percent * mental_health['PopulationCount']
poormentalHealth = round(poormentalHealth)
poormentalHealth = poormentalHealth.rename('TotalPoorMentalHealth')
mental_health = pd.concat([mental_health, poormentalHealth], axis=1)

# Drops all rows that contain null values
mental_healthDf = mental_health.dropna()

mental_healthDf


# Select Index
heart_disease = heartDisease_df[["StateDesc",'CityName', "Data_Value", "PopulationCount","Measure"]]

#convert data value (%) to real number & add to dataframe
Mpercent = heart_disease["Data_Value"]*0.01
CHeartDisease = Mpercent * heart_disease['PopulationCount']
CHeartDisease = round(CHeartDisease)
CHeartDisease = CHeartDisease.rename('TotalHeartDisease')
heart_disease = pd.concat([heart_disease, CHeartDisease], axis=1)

# Drops all rows that contain null values
heart_diseaseDf = heart_disease.dropna()


heart_diseaseDf





# Calculate total number of states
total_states = heart_diseaseDf.StateDesc.nunique()
# Calculate total number of cities
total_cities = heart_diseaseDf.CityName.nunique()
# Calculate the total population
population = heart_diseaseDf.PopulationCount.sum()
# Calculate the average heart disease count
heart_disease = heart_diseaseDf.TotalHeartDisease.sum()

avgheart_disease = heart_disease.mean()
#Calulate Total heart Disease Count
totalHeart_disease = heart_disease.sum()
# Calculate the percentage of adults 18+ with Heart Disease
percent_heart = totalHeart_disease/population 


heartDisease_summary = pd.DataFrame({"Total States" : [total_states], 'Total Cities' : [total_cities], 
                        "Total Population" : [population], "Average Heart Disease Count" : [avgheart_disease], 
                        "Total Heart Disease Count" : [totalHeart_disease], "% Adults with Heart Disease": [percent_heart]})
#formatting
heartDisease_summary["% Adults with Heart Disease"] = heartDisease_summary["% Adults with Heart Disease"].map("{:.2%}".format)

heartDisease_summary            





# Calculate total number of states
Mtotal_states = mental_healthDf.StateDesc.nunique()
# Calculate total number of cities
Mtotal_cities = mental_healthDf.CityName.nunique()
# Calculate the total population
Mpopulation = mental_healthDf.PopulationCount.sum()
# Calculate average Mental Health not good for >=14 days
mental_health = mental_healthDf.TotalPoorMentalHealth.sum()

avgmental_health = mental_health.mean()
#Calulate Total heart Disease Count
totalMental_health = mental_health.sum()
# Calculate the percentage of adults 18+ with not good mental health >=14 days
percent_mental = totalMental_health/Mpopulation


mentalHealth_summary = pd.DataFrame({"Total States" : [Mtotal_states], 'Total Cities' : [Mtotal_cities], 
                        "Total Population" : [Mpopulation], "Average Poor Mental Health Count" : [avgmental_health], 
                        "Total Adults with Poor Mental Health" : [totalMental_health], "% Adults with Poor Mental Health": [percent_mental]})
#formatting
mentalHealth_summary["% Adults with Poor Mental Health"] = mentalHealth_summary["% Adults with Poor Mental Health"].map("{:.2%}".format)

mentalHealth_summary    





# Using the aggregation method, produce summary statistics for heart disease per state
heart_diseaseDf.groupby(['StateDesc'])['TotalHeartDisease'].aggregate(['mean', 'median', 'var', 'std', 'sem'])


# Using the aggregation method, produce summary statistics for poor mental health per state
mental_healthDf.groupby(['StateDesc'])['TotalPoorMentalHealth'].aggregate(['mean', 'median', 'var', 'std', 'sem'])





# 10 Best Mental Health Cities
high_mental = mental_healthDf.sort_values('Data_Value')
high_mental.head(10)


# 10 Lowest Heart Disease Cities
lowest_heartDisease = heart_diseaseDf.sort_values('Data_Value')
lowest_heartDisease.head(10)





# 10 Worst Mental Health`Cities
poor_mental = mental_healthDf.sort_values('Data_Value', ascending=False)
poor_mental.head(10)


# 10 Highest Heart Disease Cities
bottom_heartDisease = heart_diseaseDf.sort_values('Data_Value', ascending=False)
bottom_heartDisease.head(10)





# Group State & Mental Health Data
mentalhealth_bar = mental_healthDf.groupby(['StateDesc'])['Data_Value'].mean()

plt.figure(figsize=(10,6))
# plot the group in decsending order 
mentalhealth_bar.sort_values(ascending=False).plot.bar(color='g')


# Label x,y axis
plt.xlabel('States')
plt.ylabel('Mental health not good >=14 days (%)')
plt.title('Poor Mental Health Average Rates by State')

#resize/rescale
plt.xticks(fontsize=7.5)
plt.ylim(8)

plt.show()





# Group State & Mental Health Data
heartdisease_bar = heart_diseaseDf.groupby(['StateDesc'])['Data_Value'].mean()

plt.figure(figsize=(10, 6))
# plot the group in decsending order 
heartdisease_bar.sort_values(ascending=False).plot.bar()

# Label x,y axis & title
plt.xlabel('States')
plt.ylabel('Heart disease among adults aged >=18 (%)')
plt.title('Heart Disease Average Rates by State')

#resize/rescale
plt.ylim(3)
plt.xticks(fontsize=8)

plt.show()





plt.figure(figsize=(6, 10))
mylabels=['Average Heart Disease', 'Average Mental Health']
# plot the group in decsending order 
pd.concat([heartdisease_bar, mentalhealth_bar], axis=1).plot.bar(color=('b','g'))



# Label x,y axis, legend & title
plt.xlabel('States')
plt.ylabel('Average Heart Disease & Poor Mental health (%)')
plt.title('Heart Disease & Mental Health among adults aged >=18')
plt.legend(labels=mylabels)

#resize/rescale
plt.xticks(fontsize=8)
plt.ylim(3)

plt.show()


# scatter plot of heart disease vs mental health
plt.scatter(mentalhealth_bar, heartdisease_bar)
# find p values
p_slope, p_int, p_r, p_p, p_std_err = stats.linregress(mentalhealth_bar, heartdisease_bar)
p_fit = p_slope*mentalhealth_bar+p_int

#plot slope line & label x,y and title
plt.plot(mentalhealth_bar, p_fit, color = "purple")
plt.xlabel('Average Mental Health (%)')
plt.ylabel('Average Heart Disease (%)')
plt.title("Average Heart Disease by Mental Health")

#print analysis
print(f'The correlation coefficient between Heart Disease & Mental Health is {round(p_r,2)}')
print(f'The correlation between Heart Disease & Mental Health is Strong')

plt.show()






